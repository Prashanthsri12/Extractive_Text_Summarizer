{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_summary(data,top_n):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    summary=[]\n",
    "    for X in data:\n",
    "\n",
    "        #----------------------------------------------------\n",
    "        #reading sentence\n",
    "        \n",
    "        article = nltk.tokenize.sent_tokenize(X)\n",
    "        sentences = []\n",
    "        \n",
    "        for sent in article:\n",
    "\n",
    "            sentences.append(sent.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "        sentences.pop() \n",
    "        \n",
    "        #print(sentences)\n",
    "        #----------------------------------------------------\n",
    "        #sentence similarity\n",
    "\n",
    "        def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "            if stopwords is None:\n",
    "                stopwords = []\n",
    "\n",
    "            sent1 = [w.lower() for w in sent1]\n",
    "            sent2 = [w.lower() for w in sent2]\n",
    "            #print(sent1,sent2)\n",
    "            all_words = list(set(sent1 + sent2))\n",
    "\n",
    "            vector1 = [0] * len(all_words)\n",
    "            vector2 = [0] * len(all_words)\n",
    "\n",
    "            # build the vector for the first sentence\n",
    "            for w in sent1:\n",
    "                if w in stopwords:\n",
    "                    continue\n",
    "                vector1[all_words.index(w)] += 1\n",
    "\n",
    "            # build the vector for the second sentence\n",
    "            for w in sent2:\n",
    "                if w in stopwords:\n",
    "                    continue\n",
    "                vector2[all_words.index(w)] += 1\n",
    "            #print(all_words)\n",
    "            #print(len(all_words))\n",
    "            return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "        #print(\"sentence_similarity :\",sentence_similarity,'\\n','-'*50)\n",
    "            \n",
    "\n",
    "        #-----------------------------------------------------------------------------\n",
    "        #similarity matrix\n",
    "\n",
    "        similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "        for idx1 in range(len(sentences)):\n",
    "                for idx2 in range(len(sentences)):\n",
    "                    if idx1 == idx2: #ignore if both are same sentences\n",
    "                        continue \n",
    "                    similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "\n",
    "        #print(\"similarity matrix :\\n\",similarity_matrix,'\\n','-'*50)\n",
    "\n",
    "        #------------------------------------------------------------------------------\n",
    "        #ranking sentence \n",
    "\n",
    "        sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(sentence_similarity_graph)\n",
    "        ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True) \n",
    "        #print(\"ranked_sentence :\",ranked_sentence,'\\n','-'*50)\n",
    "\n",
    "        #--------------------------------------------------------------------------------\n",
    "        #summarize text\n",
    "        summarize_text = []\n",
    "\n",
    "\n",
    "        #top_n = 15\n",
    "        for i in range(top_n):\n",
    "            if X != '-' :\n",
    "                summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "            else :\n",
    "                summarize_text.append('-')\n",
    "        #print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "        #print('-'*110)\n",
    "\n",
    "        summary.append(\". \".join(summarize_text))\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with real news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article = '''Data science and big data are often viewed in concert, but data science can be used to extract value from data of all sizes, whether structured, unstructured, or semi-structured. Of course, big data is useful to data scientists in many cases, because the more data you have, the more parameters you can include in a given model.\n",
    "\n",
    "\"With big data, you're not necessarily bound to the dimensionality constraints of small data,\" Hunt says. \"Big data does help in certain aspects, but more isn't always better. If you take the stock market and try to fit it to a line, it's not going to work. But maybe, if you only look at it for a day or two, you can set it to a line.\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Of course, big data is useful to data scientists in many cases, because the more data you have, the more parameters you can include in a given model.. Data science and big data are often viewed in concert, but data science can be used to extract value from data of all sizes, whether structured, unstructured, or semi-structured.. \"Big data does help in certain aspects, but more isn\\'t always better.']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news1_summary = data_summary(pd.Series([Article]),3)\n",
    "news1_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
